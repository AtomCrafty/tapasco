%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Programming Interfaces}\label{sec:apis}%
This chapter discusses the \gloss{TPC API} and the \gloss{Platform API}, the two central abstractions on the software layer, in detail.
Each \gloss{Architecture} and \gloss{Platform} will need to provide suitable custom implementations.
Note that implementation of the \gloss{Platform API} is only required for FPGA devices, for DSP devices the implementation of \gloss{TPC API} suffices.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{TPC API}\label{sec:ai-tapascoapi}%
On the software side, each \gloss{Architecture} implementation has to provide a shared library implementing the \gloss{TPC API} defined in \code{arch/common/include/tapasco\_api.h} (for the full listing see \lstref{lst:tapasco_api.h}).
This section gives a short overview of \gloss{TPC API} and the general idea behind its design, please refer to the header file for implementation details.

\paragraph{Overview}
\gloss{TPC API} is the \emph{uniform programming interface} for hardware threadpool devices (such as FPGAs, DSPs).
It provides functions to manage and query devices, prepare and launch \gloss{Jobs}, and collect the resulting data.
It is therefore concerned with the part of the design marked "Threadpool" in \figref{fig:arch-ports} and should hide all design decisions specific to the \gloss{Architecture} implementation, e.g., the internal address map of hardware threads.
To interface with the platform (e.g., to issue reads or writes) it should use the \gloss{Platform API} (described in \secref{sec:pi-papi}).
Many API functions use a \code{flags} argument to pass additional flags to the implementation, which may be defined and interpreted by the implementation.
E.g., FPGAs and DSPs may define and use different flags for calls to \code{tapasco\_device\_alloc}.
In general, the API supports most calls in \emph{two different modes}:
%
\begin{itemize}
  \item \emph{blocking mode} ---  i.e., the called function will only return the call has finished successfully and may wait arbitrary time, e.g., to wait for scarce resources to become available.
  \item \emph{non-blocking mode} --- i.e., the called function will always return as quickly as possible and may return an error in case, e.g., when a scarce resource was not available.
\end{itemize}
%
An implementation of \gloss{TPC API} is not required to support both modes, and may return an appropriate error code accordingly.
\paragraph{Logical Organization of Threadpools}
The underlying organization of the threadpool (e.g., number of instances of each \gloss{Kernel}) is hidden from the \gloss{TPC API} user, instead applications request \emph{scheduling} of a \gloss{Job} for a \gloss{Kernel ID}, which has the type \code{tapasco\_func\_id\_t} and uniquely identifies a \gloss{Kernel} (not an instance).
The semantics of this \gloss{Kernel ID} are currently of \emph{purely transitory nature} for \tapasco{}, i.e., they can be freely defined by the user outside of the flow.
The current mechanism to determine which \gloss{Kernels} are instantiated and how often in a bitstream is based on an environment variable (\code{TPC\_COMPOSITION}), which simply contains a sequence of \gloss{Kernel IDs} separated by spaces.
An implementation of \gloss{TPC API} (specifically \code{tapasco\_\allowbreak load\_\allowbreak bitstream}) can, e.g.,  set this variable using a user-provided ID table or similar provision.
Future \gloss{Architecture} implementations could also use custom hardware to carry this meta-information within the bitstream itself and make it accessible.

\paragraph{Querying a Device}
A threadpool devices' currently active \gloss{Composition} can be queried using \gloss{TPC API} calls:
The type \code{tapasco\_func\_id\_t} uniquely identifies a \gloss{Kernel} in this context, its semantics are defined by the application (i.e., the application controls the identification of kernels via this type).
If the number of instances of a queried \gloss{Kernel} is greater than zero, then \gloss{Jobs} for it may be scheduled on this device, otherwise the \gloss{Kernel} is not available.
An exemplary minimal function management is implemented in \code{arch/\allowbreak common/\allowbreak src/\allowbreak tapasco\_functions.c} and can be re-used in new implementations and/or device drivers.

\paragraph{Preparing and Launching Jobs}
%
\begin{figure}
  \centering\includegraphics[]{tikz/tapasco-api-job-exec}
  \caption{Flow Chart for executing a job via TPC API}
  \label{fig:tapasco-api-job-exec}
\end{figure}
%
\gloss{TPC API} provides several functions to manage \gloss{Jobs}, i.e., argument value sets for a \gloss{Kernel} execution.
\gloss{Jobs} are identified by the type \code{tapasco\_job\_id\_t}, the semantics of which are defined by the \gloss{TPC API} implementation and should be treated as an opaque value by the application.
The overall flow for the execution of a job is depicted in \figref{fig:tapasco-api-job-exec}:
The first step is to prepare large data transfer by allocating memory on the device and triggering the transfer.
Then a \gloss{Job ID} must be acquired and prepared for launch by setting value arguments (e.g., device memory handles, \ldots).
The \gloss{Job} is then scheduled for launch; afterwards, the direct return arguments can be retrieved (if any) and the \gloss{Job ID} is released.
Finally, if required, memory can be fetched back from the device and the previously allocated handles must be released.
An exemplary minimal job management is implemented in \code{arch/\allowbreak common/\allowbreak src/\allowbreak tapasco\_jobs.c} and can be re-used in new implementations and/or device drivers.

\paragraph{Memory Allocation and Data Transfer}
Direct \gloss{Kernel} arguments of limited size are gathered in \gloss{Jobs} and are handled automatically by the \gloss{TPC API} implementation.
For larger data, separate memory allocation and data transfer functions are defined in \gloss{TPC API} and must be implemented by new \gloss{Architectures}.
Asynchronous memory transfers (i.e., started in non-blocking mode) are not guaranteed to finish in-order, but for convenience, synchronous memory transfers (i.e., started in blocking mode) act as \emph{barrier} --- all asynchronous transfers started previously are guaranteed to finish before the call returns.
Any \gloss{TPC API} implementation supporting non-blocking mode for memory transfers must implement this behavior.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Platform API}\label{sec:pi-papi}%
On the software side, each \gloss{Platform} implementation has to provide a shared library implementing the simple \gloss{Platform API} (cf. \figref{fig:two-tiered-design-abstraction}) defined in \code{platform/\allowbreak common/\allowbreak include/\allowbreak platform\_api.h} (for the full listing see \lstref{lst:platform_api.h}).
The \gloss{Platform API} provides functions \emph{read and write device memory}, \emph{read and write threadpool hardware registers} and a \emph{generic signaling / notification mechanism} that informs the host of platform events, e.g., completion of \gloss{Jobs}.
Furthermore, there are some optional methods which may be implemented for simulation (see end of \lstref{lst:platform_api.h}).

\paragraph{Hardware Registers and Memory Access}
\gloss{Platform API} defines two \emph{logical address spaces}: the type \code{platform\_ctl\_addr\_t} represents addresses in the \emph{AXI4 slave address space} (i.e., the IP connected via \code{arch\_get\_slaves}), whereas \code{platform\_mem\_addr\_t} represents addresses in the \emph{device memory space}.
The former is used to control the threadpool and its threads, the latter is used for large memory transfers and usually corresponds to the \code{tapasco\_handle\_t} defined in \gloss{TPC API}.
However, these types should be treated as opaque and new \gloss{Platform} implementations are not required to implement them as disjoint address spaces.

\paragraph{Signaling / Notification Mechanism}
\gloss{Platform API} only defines a very primitive signaling mechanism based on generic event numbers and callback functions:
Functions of the type \code{platform\_irq\_callback\_t} may be registered via \code{platform\_register\_irq\_callback} and are called whenever an \emph{event} happens.
Events are identified via numbers; semantics are platform-defined, e.g., the sample \code{zynq} platform uses the first 48 events for the interrupt signals of the corresponding hardware threads.
However, the mechanism is not limited to interrupts and can be used to create richer event systems, which can be particularly useful in simulation.

